{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0664aac9-594d-437f-bba6-8b7a465e68ef",
   "metadata": {},
   "source": [
    "# Audio Deepfake Detection Using ECAPA-TDNN Model\n",
    "This notebook demonstrates the complete workflow for detecting audio deepfakes using a pretrained ECAPA-TDNN model and a custom classifier.\n",
    "\n",
    "Dataset : https://drive.google.com/drive/folders/18Wz86no7pKuXLqo0LKCF-_wQx3R-nf83?usp=sharing\n",
    "\n",
    "( Above data is preprocessed and the preprocesssing code is in preprocessing_data_ecapa.ipynb notebook )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9793aae-0ec1-45a6-b23f-96ba3251f645",
   "metadata": {},
   "source": [
    "## Installation of Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85710a4b-8f8e-4183-acc9-c41d58a7a327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./env/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchaudio in ./env/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: librosa in ./env/lib/python3.8/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: speechbrain in ./env/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.8/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.8/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.8/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./env/lib/python3.8/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./env/lib/python3.8/site-packages (from librosa) (1.23.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./env/lib/python3.8/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./env/lib/python3.8/site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in ./env/lib/python3.8/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./env/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./env/lib/python3.8/site-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./env/lib/python3.8/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./env/lib/python3.8/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./env/lib/python3.8/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./env/lib/python3.8/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./env/lib/python3.8/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: hyperpyyaml in ./env/lib/python3.8/site-packages (from speechbrain) (1.2.2)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.8/site-packages (from speechbrain) (24.1)\n",
      "Requirement already satisfied: sentencepiece in ./env/lib/python3.8/site-packages (from speechbrain) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.8/site-packages (from speechbrain) (4.66.5)\n",
      "Requirement already satisfied: huggingface-hub in ./env/lib/python3.8/site-packages (from speechbrain) (0.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./env/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in ./env/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: importlib-metadata in ./env/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (7.0.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./env/lib/python3.8/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./env/lib/python3.8/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./env/lib/python3.8/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./env/lib/python3.8/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.8/site-packages (from huggingface-hub->speechbrain) (6.0.2)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in ./env/lib/python3.8/site-packages (from hyperpyyaml->speechbrain) (0.18.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in ./env/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in ./env/lib/python3.8/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in ./env/lib/python3.8/site-packages (from importlib-metadata->numba>=0.51.0->librosa) (3.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchaudio librosa speechbrain pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6deb27-4aaf-46c8-8dbe-e7d5bc0a63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec46222-d42d-4c1d-a23f-3cf19dd82880",
   "metadata": {},
   "source": [
    "## Data Preparation and Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bcb41-c00c-4981-a35a-6d855b74766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample, MelSpectrogram\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset class for Audio Deepfake Detection\n",
    "class AudioDeepfakeDataset(Dataset):\n",
    "    def __init__(self, folder_path, labels_file, target_sr=16000):\n",
    "        \"\"\"\n",
    "        Initializes the dataset for audio deepfake detection.\n",
    "        \n",
    "        Args:\n",
    "            folder_path (str): Path to the folder containing audio files.\n",
    "            labels_file (str): Path to the CSV file with filenames and labels.\n",
    "            target_sr (int): Target sample rate for audio files.\n",
    "        \"\"\"\n",
    "        self.folder_path = folder_path\n",
    "        self.labels = pd.read_csv(labels_file)\n",
    "        self.target_sr = target_sr\n",
    "        self.mel_transform = MelSpectrogram(\n",
    "            sample_rate=target_sr,\n",
    "            n_fft=400,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=80\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the Mel spectrogram and label for a given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Mel spectrogram of the audio.\n",
    "            int: Label (0 for bonafide, 1 for spoof).\n",
    "        \"\"\"\n",
    "        # Get file name and label\n",
    "        row = self.labels.iloc[idx]\n",
    "        file_path = os.path.join(self.folder_path, row['filename'])\n",
    "        label = 1 if row['label'] == \"spoof\" else 0  # Encode labels: spoof = 1, bonafide = 0\n",
    "\n",
    "        # Load and resample audio\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != self.target_sr:\n",
    "            resampler = Resample(orig_freq=sr, new_freq=self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Apply Mel spectrogram transform (feature extraction)\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "\n",
    "        return mel_spec, label\n",
    "\n",
    "\n",
    "# Paths to your dataset and CSV labels\n",
    "train_folder = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/train\"\n",
    "train_labels = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/train_labels.csv\"\n",
    "dev_folder = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/dev\"\n",
    "dev_labels = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/dev_labels.csv\"\n",
    "eval_folder = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/eval\"\n",
    "eval_labels = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/eval_labels.csv\"\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AudioDeepfakeDataset(train_folder, train_labels)\n",
    "dev_dataset = AudioDeepfakeDataset(dev_folder, dev_labels)\n",
    "eval_dataset = AudioDeepfakeDataset(eval_folder, eval_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a2e78-e5a4-4f6e-a270-f082cb0ce546",
   "metadata": {},
   "source": [
    "## Pretrained ECAPA-TDNN Model Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b666dc67-47e4-42ff-8aef-73b75f657415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/var/folders/bm/pv51_gj91jvdykfwty90ptrw0000gn/T/ipykernel_1275/879187341.py:1: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SpeakerRecognition\n",
      "/var/folders/bm/pv51_gj91jvdykfwty90ptrw0000gn/T/ipykernel_1275/879187341.py:4: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "/Users/samruddhikale/tensorflow-cair/env/lib/python3.8/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/samruddhikale/tensorflow-cair/env/lib/python3.8/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Users/samruddhikale/tensorflow-cair/env/lib/python3.8/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "\n",
    "# Load pretrained ECAPA-TDNN model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n",
    "classifier = SpeakerRecognition.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"tmp/speechbrain_ecapa\",\n",
    "    run_opts={\"device\": device}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3cfd6-8d45-4851-98b2-2fb8c0908a39",
   "metadata": {},
   "source": [
    "##  Custom Classifier for Deepfake Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f0c23d-092a-4474-b699-24e9093e982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepfakeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepfakeClassifier, self).__init__()\n",
    "        \n",
    "        # Add hidden layers and dropout\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.batch_norm = nn.BatchNorm1d(512)  # Batch Normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batch_norm(x)  # Apply batch normalization\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # Output logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adf50b-f33d-4e81-a888-f69015237b5f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50563f54-1e31-4aba-97de-67f66a6bd835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3667, Accuracy: 83.36%\n",
      "Epoch [2/10], Loss: 0.2701, Accuracy: 88.74%\n",
      "Epoch [3/10], Loss: 0.2269, Accuracy: 90.61%\n",
      "Epoch [4/10], Loss: 0.2123, Accuracy: 91.59%\n",
      "Epoch [5/10], Loss: 0.1839, Accuracy: 92.72%\n",
      "Epoch [6/10], Loss: 0.1744, Accuracy: 93.09%\n",
      "Epoch [7/10], Loss: 0.1607, Accuracy: 93.85%\n",
      "Epoch [8/10], Loss: 0.1491, Accuracy: 94.27%\n",
      "Epoch [9/10], Loss: 0.1431, Accuracy: 94.58%\n",
      "Epoch [10/10], Loss: 0.1301, Accuracy: 94.98%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classification model\n",
    "input_dim = 80 * 301  # Number of Mel bands * time steps (for a single frame of audio)\n",
    "deepfake_classifier = DeepfakeClassifier(input_dim).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "optimizer = optim.Adam(deepfake_classifier.parameters(), lr=0.0001)  # Reduce learning rate\n",
    "\n",
    "# Training loop with accuracy\n",
    "for epoch in range(10):\n",
    "    deepfake_classifier.train()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    for mel_spec, labels in train_loader:\n",
    "        mel_spec, labels = mel_spec.to(device), labels.to(device)\n",
    "\n",
    "        # Flatten the Mel spectrogram\n",
    "        mel_spec = mel_spec.view(mel_spec.size(0), -1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = deepfake_classifier(mel_spec).squeeze(1)  # No sigmoid applied here\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Predictions\n",
    "        predictions = torch.sigmoid(outputs) >= 0.5  # Apply sigmoid, then threshold\n",
    "\n",
    "        # Accuracy calculation\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # This is the correct method to call\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate epoch loss and accuracy\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9b509-ef13-4cb9-a9f2-b4f9cf0ec910",
   "metadata": {},
   "source": [
    "## Model Saving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04430876-9803-4ea3-918b-3f1d2672ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /Users/samruddhikale/tensorflow-cair/model/deepfake_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "import torch  # Make sure torch is imported\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"/Users/samruddhikale/tensorflow-cair/model/deepfake_classifier.pth\"  # Change this path to where you want to save the model\n",
    "torch.save(deepfake_classifier.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d8bfe-4b5d-4cb7-9dac-c3fba98a5639",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943dc39a-9193-48be-ac4f-51982137cbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 1.2472, Evaluation Accuracy: 93.13%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "deepfake_classifier.eval()  # Set the model to evaluation mode\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "running_loss = 0\n",
    "\n",
    "with torch.no_grad():  # No gradients needed during evaluation\n",
    "    for mel_spec, labels in eval_loader:\n",
    "        mel_spec, labels = mel_spec.to(device), labels.to(device)\n",
    "\n",
    "        # Flatten the Mel spectrogram\n",
    "        mel_spec = mel_spec.view(mel_spec.size(0), -1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = deepfake_classifier(mel_spec).squeeze(1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Predictions\n",
    "        predictions = torch.sigmoid(outputs) >= 0.5  # Apply sigmoid, then threshold\n",
    "\n",
    "        # Accuracy calculation\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "# Calculate evaluation loss and accuracy\n",
    "eval_loss = running_loss / len(eval_loader)\n",
    "eval_accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Evaluation Loss: {eval_loss:.4f}, Evaluation Accuracy: {eval_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec3bc9-6a13-4db8-82ba-4335b41f3c6b",
   "metadata": {},
   "source": [
    "## Predicting on New Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231112ad-5d7f-4c84-b231-545364a0ce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the audio file: spoof\n"
     ]
    }
   ],
   "source": [
    "# Function for making predictions on new data\n",
    "def predict(model, audio_path, target_sr=16000):\n",
    "    # Load and preprocess the audio file\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    if sr != target_sr:\n",
    "        waveform = Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n",
    "\n",
    "    # Apply Mel spectrogram transform\n",
    "    mel_transform = MelSpectrogram(\n",
    "        sample_rate=target_sr, n_fft=400, win_length=400, hop_length=160, n_mels=80\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "\n",
    "    # Flatten the Mel spectrogram\n",
    "    mel_spec = mel_spec.reshape(1, -1)  # Add batch dimension\n",
    "\n",
    "    # Move the data to the same device as the model\n",
    "    mel_spec = mel_spec.to(device)\n",
    "\n",
    "    # Model prediction\n",
    "    with torch.no_grad():  # No gradients needed for inference\n",
    "        output = model(mel_spec).squeeze(1)\n",
    "\n",
    "    # Apply sigmoid to the output and threshold at 0.5 for binary classification\n",
    "    prediction = torch.sigmoid(output) >= 0.5\n",
    "    return \"spoof\" if prediction.item() == 1 else \"bonafide\"\n",
    "\n",
    "# Example: Use the model to predict on a new audio file\n",
    "audio_file = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/dev/LA_T_2312160.wav\"  # Replace with the actual path to your new audio file\n",
    "prediction = predict(deepfake_classifier, audio_file)\n",
    "print(f\"Prediction for the audio file: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0c821-21f4-4ee2-9c4b-3ad74ff14644",
   "metadata": {},
   "source": [
    "## Sampling and Testing on Smaller Data Subset\n",
    "Randomly samples 500 files from the unknown data for quick testing.\n",
    "Saves the subset and corresponding labels to new folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f534cd8e-627f-400f-bf69-1f206f5ca747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 500 files have been copied to /Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/new_dev.\n",
      "Corresponding labels CSV has been saved to /Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/new_dev_labels.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "dev_folder = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/dev\"\n",
    "dev_labels_file = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/dev_labels.csv\"\n",
    "new_folder = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/new_dev\"\n",
    "new_labels_file = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/new_dev_labels.csv\"\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "# Load the dev CSV\n",
    "dev_labels = pd.read_csv(dev_labels_file)\n",
    "\n",
    "# Randomly sample 100 rows\n",
    "sampled_labels = dev_labels.sample(n=500, random_state=42)  # Use random_state for reproducibility\n",
    "\n",
    "# Copy the sampled files to the new folder\n",
    "for _, row in sampled_labels.iterrows():\n",
    "    source_path = os.path.join(dev_folder, row['filename'])\n",
    "    destination_path = os.path.join(new_folder, row['filename'])\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "# Save the sampled labels to a new CSV\n",
    "sampled_labels.to_csv(new_labels_file, index=False)\n",
    "\n",
    "print(f\"Randomly selected 500 files have been copied to {new_folder}.\")\n",
    "print(f\"Corresponding labels CSV has been saved to {new_labels_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a30c2-06de-48f4-a8a7-6d022d07a846",
   "metadata": {},
   "source": [
    "## Predictions and Accuracy\n",
    "Compares predictions with ground truth labels from the new dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99428c83-72f4-409f-9d59-3b64138bfafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/predictions.csv\n",
      "Accuracy on the 100 files: 93.40%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "\n",
    "# Function for making predictions on an audio file\n",
    "def predict(model, audio_path, target_sr=16000):\n",
    "    # Load and preprocess the audio file\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != target_sr:\n",
    "        waveform = Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n",
    "\n",
    "    # Apply Mel spectrogram transform\n",
    "    mel_transform = MelSpectrogram(\n",
    "        sample_rate=target_sr, n_fft=400, win_length=400, hop_length=160, n_mels=80\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "\n",
    "    # Flatten the Mel spectrogram\n",
    "    mel_spec = mel_spec.reshape(1, -1)  # Add batch dimension\n",
    "\n",
    "    # Move the data to the same device as the model\n",
    "    mel_spec = mel_spec.to(device)\n",
    "\n",
    "    # Model prediction\n",
    "    with torch.no_grad():  # No gradients needed for inference\n",
    "        output = model(mel_spec).squeeze(1)\n",
    "\n",
    "    # Apply sigmoid to the output and threshold at 0.5 for binary classification\n",
    "    prediction = torch.sigmoid(output) >= 0.5\n",
    "    return \"spoof\" if prediction.item() == 1 else \"bonafide\"\n",
    "\n",
    "# Paths to the new folder and its CSV file\n",
    "new_folder = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/new_dev\"\n",
    "new_labels_file = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/new_dev_labels.csv\"\n",
    "predictions_file = \"/Users/samruddhikale/Desktop/CAIR/OG1/Preprocessed_Dataset/predictions.csv\"\n",
    "\n",
    "# Load the new labels CSV\n",
    "new_labels = pd.read_csv(new_labels_file)\n",
    "\n",
    "# Create a list to store prediction results\n",
    "results = []\n",
    "\n",
    "# Loop through the files in the new folder\n",
    "for _, row in new_labels.iterrows():\n",
    "    audio_path = os.path.join(new_folder, row[\"filename\"])\n",
    "    label = row[\"label\"]  # Ground truth label\n",
    "    prediction = predict(deepfake_classifier, audio_path)  # Model prediction\n",
    "    results.append({\"filename\": row[\"filename\"], \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "predictions_df = pd.DataFrame(results)\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {predictions_file}\")\n",
    "\n",
    "# Compare predictions with ground truth\n",
    "correct = (predictions_df[\"label\"] == predictions_df[\"prediction\"]).sum()\n",
    "accuracy = correct / len(predictions_df) * 100\n",
    "print(f\"Accuracy on the 100 files: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9d36d-cac6-42db-be00-44b59db8a77c",
   "metadata": {},
   "source": [
    "#### This notebook provides a structured pipeline for audio deepfake detection, combining feature extraction, a pretrained model, and a custom classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62744a72-b075-4c68-b420-f610dd738393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
